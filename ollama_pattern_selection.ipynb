{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "pip install ollama\n"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-28T15:02:44.149935Z",
     "iopub.status.busy": "2024-11-28T15:02:44.149036Z",
     "iopub.status.idle": "2024-11-28T15:02:46.988655Z",
     "shell.execute_reply": "2024-11-28T15:02:46.987904Z",
     "shell.execute_reply.started": "2024-11-28T15:02:44.149875Z"
    },
    "trusted": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load input file"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "project_title_map = {\n",
    "        \"archives_space\": \"Archives Space Project\",\n",
    "        \"bad_camp\": \"BADCamp Project\",\n",
    "        \"neurohub\": \"NeuroHub Project\",\n",
    "        \"open_spending\": \"Open Spending Project\",\n",
    "        \"planning_poker\": \"Planning Poker Project\",\n",
    "        \"recycling\": \"Recycling Project\"\n",
    "    }\n",
    "projects = [\"archives_space\", \"bad_camp\", \"neurohub\", \"open_spending\", \"planning_poker\", \"recycling\"]\n",
    "\n",
    "selected_project = projects[5] # select which project you want to test with\n",
    "\n",
    "project_path = \"./input_files/user_stories_{}.json\".format(selected_project)\n",
    "with open(project_path, 'r') as file:\n",
    "        project_content = file.read()\n",
    "        project_title = project_title_map[selected_project]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load model and configure model parameters"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import ollama\n",
    "from ollama import Client\n",
    "\n",
    "model_version = \"llama3.1:8b-instruct-fp16\"\n",
    "#model_version = \"llama3.1:70b\"\n",
    "num_ctx = 20480 # context length is higher because of the refinement process\n",
    "temperature = 0 # should be kept 0 for deterministic results, default value 0.8\n",
    "\n",
    "options = {\"num_ctx\": num_ctx, \"temperature\": temperature}\n",
    "#model_name = \"emir_\" + model_version\n",
    "model_name = model_version\n",
    "#ollama.create(model=model_name, modelfile=modelfile)\n",
    "\n",
    "one_shot = False"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load one-shot example if set True"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import json\n",
    "if one_shot:\n",
    "    with open('example_reasoning_1.txt', 'r') as file:\n",
    "        example_run_text = file.read()\n",
    "        example_run_prompt = f\"\"\"\n",
    "        I will give you an example run for another software project just to show you the reasoning process and the output format. \n",
    "        VERY IMPORTANT NOTE: \"NEVER COPY THE REASONING GIVEN IN THE EXAMPLE RUN! This Example is for Reference Only, Come Up With Your Own Reasoning for The User Input!\"\n",
    "        - EXAMPLE RUN START - \n",
    "\n",
    "        {example_run_text}\n",
    "        \n",
    "        - EXAMPLE RUN END -\n",
    "\n",
    "        \"\"\"\n",
    "else:\n",
    "    example_run_prompt = \"\"\n",
    "example_run_prompt\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Get first assessment from model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "client = Client(\n",
    "  headers={}\n",
    ")\n",
    "messages = []\n",
    "\n",
    "\n",
    "generator_system_message = {'role': 'system', 'content': f\"\"\"\n",
    "You are a software architect. Your job is to get a list of categorized user stories with a description, analyze them in detail and\n",
    "assign a score for each architecture pattern depending on their relevance to the project and \n",
    "if it would prove useful in the implementation of the described project.\n",
    "\n",
    "When the user gives you a list of user stories with a description, analyze the given user stories, give the reasoning for each architecture pattern, and then print out the assigned scores.\n",
    "Be as objective as possible during your scoring. The decisions need to be deterministic and reproducible, regardless of the order of elements presented or any random factors.\n",
    "\n",
    "Here are the architecture patterns you will score:\n",
    "*Layered Architecture \n",
    "*Event-Driven Architecture (Pub-Sub Architecture) \n",
    "*Microkernel Architecture \n",
    "*Microservices Architecture \n",
    "*Space-Based Architecture\n",
    "*Pipeline Architecture (Pipe-Filter Architecture) \n",
    "*Client-Server Architecture\n",
    "\n",
    "Here are the score options and their corresponding meaning:\n",
    "1: \"Completely unsuitable\"\n",
    "2: \"Partially suitable\"\n",
    "3: \"Moderately applicable\"\n",
    "4: \"Well suited\"\n",
    "5: \"Perfectly aligned\"\n",
    "\n",
    "IMPORTANT: The final scores must be given in this format after the detailed reasoning for each architecture pattern:\n",
    "\n",
    "**Layered Architecture**: -integer score value-\n",
    "**Event-Driven Architecture (Pub-Sub Architecture)**: -integer score value-\n",
    "**Microkernel Architecture**: -integer score value-\n",
    "**Microservices Architecture**: -integer score value-\n",
    "**Space-Based Architecture**: -integer score value-\n",
    "**Pipeline Architecture (Pipe-Filter Architecture)**: -integer score value-\n",
    "**Client-Server Architecture**: -integer score value-\n",
    "\n",
    "{example_run_prompt}\n",
    "\n",
    "\"\"\"}\n",
    "messages.append(generator_system_message)\n",
    "\n",
    "\n",
    "messages.append({'role': 'user', 'content': f\"\"\"\n",
    "I will give you a list of categorized user stories and a description created for a software project titled {project_title}. Please analyze them in detail and give me the scoring for each architecture pattern.\n",
    "The final scores must be given in this format after the detailed reasoning for each architecture pattern:\n",
    "\n",
    "**Layered Architecture**: -integer score value-\n",
    "**Event-Driven Architecture (Pub-Sub Architecture)**: -integer score value-\n",
    "**Microkernel Architecture**: -integer score value-\n",
    "**Microservices Architecture**: -integer score value-\n",
    "**Space-Based Architecture**: -integer score value-\n",
    "**Pipeline Architecture (Pipe-Filter Architecture)**: -integer score value-\n",
    "**Client-Server Architecture**: -integer score value-\n",
    "\n",
    "Project Title: {project_title}\n",
    "\n",
    "Categorized User Stories:\n",
    "\n",
    "{project_content}\n",
    "\n",
    "\"\"\"})\n",
    "response = client.chat(model=model_name, messages=messages, options=options)\n",
    "message = response['message']\n",
    "print(message['content'])\n",
    "messages.append(message)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Self-refinement Process"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "# Let's capture the model's initial answer in a variable:\n",
    "current_answer = message['content']  # e.g. the text with reasoning + final scores\n",
    "evaluator_context = messages.copy() # this context will be used in evaluator messages\n",
    "eval_system_message = {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"\"\"Your task is changed. You are now the Evaluator. \n",
    "Your job: By taking the previously given user stories into account, evaluate the given reasoning and scores for each architecture pattern.\n",
    "\n",
    "Here are the score options and their corresponding meaning:\n",
    "1: \"Completely unsuitable\"\n",
    "2: \"Partially suitable\"\n",
    "3: \"Moderately applicable\"\n",
    "4: \"Well suited\"\n",
    "5: \"Perfectly aligned\"\n",
    "\n",
    "- Do not assume any new information about the project. The description and the user stories for the project remain the same.\n",
    "- Only suggest a refinement for an assigned score when you are sure its score is wrong and it must change. IMPORTANT: If the current score is already reasonable DO NOT make a refinement suggestion.\n",
    "- Also, go through your previous evaluations to ensure you donâ€™t repeat the same refinement suggestion for an architecture pattern more than once for the same reason.\n",
    "\n",
    "After your refinement suggestions, on a NEW line, end your response with exactly one of the two markers:\n",
    "- REFINE (if refinements are needed)\n",
    "- NO_REFINEMENT (if no refinements are needed)\n",
    "\n",
    "Do not include any additional text after that marker.\n",
    "Do not wrap it in quotes.\n",
    "\n",
    "Important:\n",
    "- Score can only take the integer values that are given as the score options, no float values.\n",
    "- If you provide refinement for any of the pattern scores, do not use NO_REFINEMENT.\n",
    "Because this will stop the whole process and the current scores will be left unrefined.\n",
    "- Use NO_REFINEMENT only if you have zero refinements to suggest.\n",
    "\"\"\"\n",
    "}\n",
    "evaluator_context.append(eval_system_message)\n",
    "\n",
    "refiner_context = messages.copy() # this context will be used in refiner messages\n",
    "\n",
    "refine_system_message = {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"\"\"Your task is changed. You are now the Refiner. \n",
    "You will be given the latest reasoning/scores for the project and the evaluator's feedback for the latest reasoning/scores.\n",
    "Make changes on the current scores according to the given refinement.\n",
    "IMPORTANT:  ONLY make a change for a score when the refinement suggestion for that score is reasonable and the assigned score must change. Otherwise keep it unchanged.\n",
    "\n",
    "When giving your output refined scores, keep the same format in the updated response (reasoning and then the updated scores).\n",
    "Be as objective as possible during your refinement. The decisions need to be deterministic and reproducible, regardless of the order of elements presented or any random factors.\n",
    "\n",
    "Here are the score options and their corresponding meaning:\n",
    "1: \"Completely unsuitable\"\n",
    "2: \"Partially suitable\"\n",
    "3: \"Moderately applicable\"\n",
    "4: \"Well suited\"\n",
    "5: \"Perfectly aligned\"\n",
    "\n",
    "- Do not assume any new information about the project. The description and the user stories for the project remain the same.\n",
    "\"\"\"\n",
    "    }\n",
    "refiner_context.append(refine_system_message)\n",
    "# We'll define how many refinement loops to allow:\n",
    "MAX_REFINEMENT_ITERATIONS = 3\n",
    "\n",
    "refinement_iterations = MAX_REFINEMENT_ITERATIONS # if not set again in the loop, it means the model went through max iterations\n",
    "for i in range(MAX_REFINEMENT_ITERATIONS):\n",
    "    # 1) Evaluate the current answer\n",
    "    \n",
    "    evaluate_prompt = {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"\"\"\n",
    "            Evaluate the current reasoning and scores by taking the original categorized user stories into account and make refinement suggestions for the current answer:\n",
    "\n",
    "            Current Answer:\n",
    "            {current_answer}\n",
    "\n",
    "            Refinement Suggestions:\n",
    "            \"\"\"\n",
    "    }\n",
    "    evaluator_context.append(evaluate_prompt)\n",
    "    eval_response = client.chat(model=model_name, messages=evaluator_context, options=options)\n",
    "    eval_message = eval_response['message']\n",
    "    eval_feedback = eval_message['content'].strip()\n",
    "    evaluator_context.append(eval_message)\n",
    "    \n",
    "    print(f\"\\n=== Evaluator Feedback (Iteration {i+1}) ===\\n{eval_feedback}\\n\")\n",
    "    \n",
    "    # 2) If the evaluator indicates no changes are needed, break out\n",
    "    if \"NO_REFINEMENT\" in eval_feedback:\n",
    "        print(\"Evaluator says no changes are needed. Stopping refinement.\")\n",
    "        refinement_iterations = i\n",
    "        break\n",
    "    \n",
    "    # 3) Otherwise, refine\n",
    "    refine_prompt = {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"\"\"\n",
    "            Analyze the previous answer and the feedback of the evaluator, and then refine your previous answer by taking the feedback of the evaluator into account. \n",
    "            Keep the same format (reasoning and then the final scores).\n",
    "\n",
    "            Previous Answer:\n",
    "            {current_answer}\n",
    "\n",
    "            Evaluator Feedback:\n",
    "            {eval_feedback}\n",
    "\n",
    "            Revised Answer:\n",
    "            \"\"\"\n",
    "    }\n",
    "    refiner_context.append(refine_prompt)\n",
    "    \n",
    "    refine_response = client.chat(model=model_name, messages=refiner_context, options=options)\n",
    "    refine_message = refine_response['message']\n",
    "    revised_answer = refine_message['content'].strip()\n",
    "    refiner_context.append(refine_message)\n",
    "    \n",
    "    print(f\"=== Refined Answer (Iteration {i+1}) ===\\n{revised_answer}\\n\")\n",
    "    \n",
    "    # Update current_answer for potential next iteration\n",
    "    current_answer = revised_answer\n",
    "\n",
    "# After the loop, current_answer holds the final refined output:\n",
    "print(\"=== Final Refined Scores & Reasoning ===\\n\", current_answer)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Summarize the evaluation result"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "refiner_context.append({'role': 'system', 'content': \"\"\"\n",
    "Ok now the evaluation process is finished. take the first assessment and the last assessment from the user. \n",
    "And just return the final scores before and after the evaluation-refinement process\n",
    "following this display format below:\n",
    "\n",
    "**Layered Architecture**: 1 -> 2\n",
    "**Event-Driven Architecture (Pub-Sub Architecture)**: 2 -> at\n",
    "**Microkernel Architecture**: 3 -> 3\n",
    "**Microservices Architecture**: 4 -> 5\n",
    "**Space-Based Architecture**: 1 -> 3\n",
    "**Pipeline Architecture (Pipe-Filter Architecture)**: 2 -> 1\n",
    "**Client-Server Architecture**: 3 -> 3\n",
    "\"\"\"})\n",
    "\n",
    "refiner_context.append({'role': 'user', 'content': f\"\"\"\n",
    "read the first assessment and the last assessment, and return the final scores before and after the evaluation-refinement process\n",
    "following this display format below:\n",
    "\n",
    "**Layered Architecture**: 1 -> 2\n",
    "**Event-Driven Architecture (Pub-Sub Architecture)**: 2 -> 2\n",
    "**Microkernel Architecture**: 3 -> 3\n",
    "**Microservices Architecture**: 4 -> 5\n",
    "**Space-Based Architecture**: 1 -> 3\n",
    "**Pipeline Architecture (Pipe-Filter Architecture)**: 2 -> 1\n",
    "**Client-Server Architecture**: 3 -> 3\n",
    "\n",
    "\n",
    "first assessment:\n",
    "{refiner_context[2][\"content\"].strip()}\n",
    "\n",
    "last assessment:\n",
    "{current_answer.strip()}\n",
    "\"\"\"})\n",
    "print(refiner_context[-1][\"content\"])\n",
    "response = client.chat(model=model_name, messages=refiner_context, options={\"num_ctx\": num_ctx, \"temperature\": 0}) # higher temperatures are too varied\n",
    "message = response['message']\n",
    "refiner_context.append(message)\n",
    "print(message[\"content\"])\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Print log to output file"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from datetime import datetime\n",
    "import json\n",
    "from pathlib import Path\n",
    "Path(\"./logs/one-shot\").mkdir(parents=True, exist_ok=True)\n",
    "Path(\"./logs/zero-shot\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "final_data = {\n",
    "        \"modelName\": model_name,\n",
    "        \"temperature\": temperature,\n",
    "        \"context_limit\": num_ctx,\n",
    "        \"projectTitle\": project_title,\n",
    "        \"selfRefinement\": False,\n",
    "        \"oneShot\": one_shot,\n",
    "        \"timestamp\": datetime.now().strftime(\"%Y%m%d_%H%M%S\"),\n",
    "        \"lastMessage\": messages[-1][\"content\"],\n",
    "        \"numberOfIterations\": 0,\n",
    "        \"messages\": [message[\"content\"] for message in messages],\n",
    "    }\n",
    "    \n",
    "# 3) Generate a filename based on model name and current timestamp\n",
    "if one_shot:\n",
    "    filename = f\"./logs/one-shot/log_{model_version}_{final_data['timestamp']}.json\"\n",
    "else:\n",
    "    filename = f\"./logs/zero-shot/log_{model_version}_{final_data['timestamp']}.json\"\n",
    "\n",
    "\n",
    "# 4) Write the conversation to a JSON file\n",
    "with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(final_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"Assesment complete. The whole conversation is saved to {filename}\")"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.10 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "interpreter": {
   "hash": "95ec9ec1504d83f612128e0fb229072f90bbb4cb09d9d5d93b5dd26e0ca2cfd1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}