{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 98,
   "source": [
    "pip install groq scikit-learn openai"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: groq in /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages (0.17.0)\n",
      "Requirement already satisfied: scikit-learn in /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages (1.6.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.10 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages (from groq) (4.12.2)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages (from groq) (4.3.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages (from groq) (2.10.2)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages (from groq) (1.9.0)\n",
      "Requirement already satisfied: sniffio in /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages (from groq) (1.3.1)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages (from groq) (0.27.0)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages (from scikit-learn) (1.13.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages (from anyio<5,>=3.5.0->groq) (1.2.0)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages (from anyio<5,>=3.5.0->groq) (2.10)\n",
      "Requirement already satisfied: certifi in /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages (from httpx<1,>=0.23.0->groq) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages (from httpx<1,>=0.23.0->groq) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages (from pydantic<3,>=1.9.0->groq) (2.27.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-28T15:02:44.149935Z",
     "iopub.status.busy": "2024-11-28T15:02:44.149036Z",
     "iopub.status.idle": "2024-11-28T15:02:46.988655Z",
     "shell.execute_reply": "2024-11-28T15:02:46.987904Z",
     "shell.execute_reply.started": "2024-11-28T15:02:44.149875Z"
    },
    "trusted": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load input file"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "source": [
    "project_title_map = {\n",
    "        \"archives_space\": \"Archives Space Project\",\n",
    "        \"archives_space_old\": \"Archives Space Project\",\n",
    "        \"neurohub\": \"NeuroHub Project\",\n",
    "        \"open_spending\": \"Open Spending Project\",\n",
    "        \"open_spending_old\": \"Open Spending Project\",\n",
    "        \"planning_poker\": \"Planning Poker Project\",\n",
    "        \"recycling\": \"Recycling Project\",\n",
    "        \"color_ide\": \"ColorIDE Project\"\n",
    "    }\n",
    "projects = [\"archives_space\", \"neurohub\", \"open_spending\", \"planning_poker\", \"recycling\", \"color_ide\"]\n",
    "\n",
    "selected_project = projects[0] # select which project you want to test with\n",
    "\n",
    "project_path = \"./input_files/user_stories_{}.json\".format(selected_project)\n",
    "with open(project_path, 'r') as file:\n",
    "        project_content = file.read()\n",
    "        project_title = project_title_map[selected_project]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load model and configure model parameters"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "source": [
    "import groq\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=\"\", base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\")\n",
    "model_version = \"gemini-2.0-flash\"\n",
    "\n",
    "\n",
    "#client = groq.Client(api_key=\"\")\n",
    "#model_version = \"llama-3.3-70b-versatile\"\n",
    "\n",
    "#client = OpenAI(api_key=\"\")\n",
    "#model_version = \"gpt-4o\"\n",
    "\n",
    "#model_version = \"llama3.1:8b-instruct-fp16\"\n",
    "\n",
    "num_ctx = 15000 # context length is higher because of the refinement process\n",
    "temperature = 0.000000001  # should be kept 0 for deterministic results, default value 0.8\n",
    "#temperature = 0\n",
    "top_p = 0.0000001\n",
    "model_name = model_version\n",
    "\n",
    "one_shot = False\n",
    "\n",
    "add_pattern_descriptions = False"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Get first assessment from model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "source": [
    "\n",
    "def get_architecture_score(project_title, project_content, architecture_pattern, model_name):\n",
    "    generator_system_message = {'role': 'system', 'content': \"\"\"\n",
    "    You are a software architect. Your job is to get a list of categorized user stories with a description, analyze them in detail and\n",
    "    assign a score for the given architecture pattern depending on their relevance to the project and \n",
    "    if it would prove useful in the implementation of the described project.\n",
    "    IMPORTANT: You MUST respond with ONLY a single digit from 1 to 5, representing the score. DO NOT provide any other text, explanation, introduction, internal reasoning or formatting.\n",
    "    IMPORTANT: The user stories are categorized and given in json format, analyze them really carefully before you answer.\n",
    "    IMPORTANT: Your answer must relate to the suitability of the given software architecture pattern to the described project.\n",
    "\n",
    "    EXAMPLE 1:\n",
    "\n",
    "    Project Title: <title>\n",
    "\n",
    "    User Stories:\n",
    "\n",
    "    <user stories>\n",
    "\n",
    "    Architecture Pattern: Layered Architecture\n",
    "\n",
    "    Score: <int>\n",
    "\n",
    "    EXAMPLE 2:\n",
    "\n",
    "    Project Title: <title>\n",
    "\n",
    "    User Stories:\n",
    "\n",
    "    <user stories>\n",
    "\n",
    "    Architecture Pattern: Microservices Architecture\n",
    "\n",
    "    Score: <int>\n",
    "    \"\"\"}\n",
    "    user_message = {'role': 'user', 'content': f\"\"\"\n",
    "    Project Title: {project_title}\n",
    "\n",
    "    User Stories:\n",
    "\n",
    "    {project_content}\n",
    "\n",
    "    Architecture Pattern: {architecture_pattern}\n",
    "\n",
    "    Score: \n",
    "    \"\"\"}\n",
    "    messages = [generator_system_message, user_message]  # Reset messages for each pattern\n",
    "\n",
    "    response = client.chat.completions.create(model=model_name, messages=messages, temperature=temperature, max_completion_tokens=1)\n",
    "    message = response.choices[0].message\n",
    "    print(message.content)\n",
    "\n",
    "    try:\n",
    "        score = int(message.content.strip())\n",
    "        if 1 <= score <= 5:\n",
    "            return score\n",
    "        else:\n",
    "            print(f\"Invalid score: {score}. Model returned a number outside the range 1-5.\")\n",
    "            return None  # Or raise an exception\n",
    "    except ValueError:\n",
    "        print(f\"Invalid response: '{message.content}'. Model did not return a valid integer.\")\n",
    "        return None  # Or raise an exception"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "source": [
    "architecture_patterns = [\"Layered Architecture\", \"Event-Driven Architecture (Pub-Sub Architecture)\",\n",
    " \"Microkernel Architecture\", \"Microservices Architecture\", \"Space-Based Architecture\", \"Pipeline Architecture (Pipe-Filter Architecture)\", \"Client-Server Architecture\"]\n",
    "\n",
    "for pattern in architecture_patterns:\n",
    "    score = get_architecture_score(project_title, project_content, pattern, model_name)\n",
    "    if score is not None:\n",
    "        print(f\"Score for {pattern}: {score}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "5\n",
      "Score for Layered Architecture: 5\n",
      "4\n",
      "Score for Event-Driven Architecture (Pub-Sub Architecture): 4\n",
      "4\n",
      "Score for Microkernel Architecture: 4\n",
      "4\n",
      "Score for Microservices Architecture: 4\n",
      "2\n",
      "Score for Space-Based Architecture: 2\n",
      "5\n",
      "Score for Pipeline Architecture (Pipe-Filter Architecture): 5\n",
      "5\n",
      "Score for Client-Server Architecture: 5\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.10 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "interpreter": {
   "hash": "95ec9ec1504d83f612128e0fb229072f90bbb4cb09d9d5d93b5dd26e0ca2cfd1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}