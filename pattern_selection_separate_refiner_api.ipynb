{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 360,
   "source": [
    "pip install groq scikit-learn openai"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: groq in /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages (0.17.0)\n",
      "Requirement already satisfied: scikit-learn in /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages (1.6.1)\n",
      "Requirement already satisfied: openai in /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages (1.61.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages (from groq) (0.27.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.10 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages (from groq) (4.12.2)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages (from groq) (4.3.0)\n",
      "Requirement already satisfied: sniffio in /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages (from groq) (1.3.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages (from groq) (1.9.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages (from groq) (2.10.2)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages (from scikit-learn) (1.13.0)\n",
      "Requirement already satisfied: tqdm>4 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages (from openai) (4.63.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages (from openai) (0.8.2)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages (from anyio<5,>=3.5.0->groq) (1.2.0)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages (from anyio<5,>=3.5.0->groq) (2.10)\n",
      "Requirement already satisfied: certifi in /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages (from httpx<1,>=0.23.0->groq) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages (from httpx<1,>=0.23.0->groq) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages (from pydantic<3,>=1.9.0->groq) (2.27.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-28T15:02:44.149935Z",
     "iopub.status.busy": "2024-11-28T15:02:44.149036Z",
     "iopub.status.idle": "2024-11-28T15:02:46.988655Z",
     "shell.execute_reply": "2024-11-28T15:02:46.987904Z",
     "shell.execute_reply.started": "2024-11-28T15:02:44.149875Z"
    },
    "trusted": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load input file"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "source": [
    "import os\n",
    "\n",
    "project_title_map = {\n",
    "        \"archives_space\": \"Archives Space Project\",\n",
    "        \"archives_space_old\": \"Archives Space Project\",\n",
    "        \"neurohub\": \"NeuroHub Project\",\n",
    "        \"open_spending\": \"Open Spending Project\",\n",
    "        \"open_spending_old\": \"Open Spending Project\",\n",
    "        \"planning_poker\": \"Planning Poker Project\",\n",
    "        \"recycling\": \"Recycling Project\",\n",
    "        \"color_ide\": \"ColorIDE Project\"\n",
    "    }\n",
    "projects = [\"archives_space\", \"neurohub\", \"open_spending\", \"planning_poker\", \"recycling\", \"color_ide\"]\n",
    "\n",
    "selected_project = os.environ.get(\"project\")\n",
    "if selected_project is None:\n",
    "    selected_project = projects[2]\n",
    "file_name = \"user_stories_{}.json\".format(selected_project)\n",
    "project_path = \"./input_files/{}\".format(file_name)\n",
    "with open(project_path, 'r') as file:\n",
    "        project_content = file.read()\n",
    "        project_title = project_title_map[selected_project]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load model and configure model parameters"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "source": [
    "import groq\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=\"\", base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\")\n",
    "model_version = \"gemini-2.0-flash\"\n",
    "\n",
    "\n",
    "#client = groq.Client(api_key=\"\")\n",
    "#model_version = \"llama-3.3-70b-versatile\"\n",
    "\n",
    "#client = OpenAI(api_key=\"\")\n",
    "#model_version = \"gpt-4o\"\n",
    "\n",
    "#model_version = \"llama3.1:8b-instruct-fp16\"\n",
    "\n",
    "num_ctx = 15000 # context length is higher because of the refinement process\n",
    "temperature = 0.000000001  # should be kept 0 for deterministic results, default value 0.8\n",
    "#temperature = 0\n",
    "top_p = 0.0000001\n",
    "model_name = model_version\n",
    "\n",
    "one_shot = False\n",
    "\n",
    "add_pattern_descriptions = True"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load one-shot example if set True"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "source": [
    "import json\n",
    "if one_shot:\n",
    "    with open('example_reasoning_1.txt', 'r') as file:\n",
    "        example_run_text = file.read()\n",
    "        example_run_prompt = f\"\"\"\n",
    "        I will give you an example run for another software project just to show you the reasoning process and the output format. \n",
    "        VERY IMPORTANT NOTE: \"NEVER COPY THE REASONING GIVEN IN THE EXAMPLE RUN! This Example is for Reference Only, Come Up With Your Own Reasoning for The User Input!\"\n",
    "        - EXAMPLE RUN START - \n",
    "\n",
    "        {example_run_text}\n",
    "        \n",
    "        - EXAMPLE RUN END -\n",
    "\n",
    "        \"\"\"\n",
    "else:\n",
    "    example_run_prompt = \"\"\n",
    "\n",
    "if add_pattern_descriptions:\n",
    "    with open('pattern_descriptions.txt', 'r') as file:\n",
    "        pattern_descriptions_text = file.read()\n",
    "else:\n",
    "    pattern_descriptions_text = \"\"\n",
    "pattern_descriptions_text\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "metadata": {},
     "execution_count": 363
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Get first assessment from model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "source": [
    "\n",
    "messages = []\n",
    "import time\n",
    "\n",
    "\n",
    "generator_system_message = {'role': 'system', 'content': f\"\"\"\n",
    "You are a software architect. Your task is to get a list of categorized user stories with a description, analyze them in detail and\n",
    "assign a score for each architecture pattern depending on their relevance to the project and \n",
    "if it would prove useful in the implementation of the described project.\n",
    "- Go over all of the user stories thoroughly, think step by step and explain your though process clearly. \n",
    "- Mention all specific user stories that helped you to make a decision.\n",
    "- Be as objective as possible during your scoring. The decisions need to be deterministic and reproducible.\n",
    "\n",
    "Here are the architecture patterns you will score:\n",
    "*Layered Architecture \n",
    "*Event-Driven Architecture\n",
    "*Microkernel Architecture \n",
    "*Microservices Architecture \n",
    "*Space-Based Architecture\n",
    "*Pipeline Architecture\n",
    "*Client-Server Architecture\n",
    "\n",
    "{pattern_descriptions_text}\n",
    "\n",
    "Here are the score options for the applicability of a pattern, ordered from lowest to highest:\n",
    "- \"Completely unsuitable\"\n",
    "- \"Partially suitable\"\n",
    "- \"Moderately applicable\"\n",
    "- \"Well suited\"\n",
    "- \"Perfectly aligned\"\n",
    "\n",
    "IMPORTANT: First explain your reasoning for each pattern, and then print the final scores in this json format:\n",
    "\n",
    "{{\n",
    "  \"Layered Architecture\": \"score option\",\n",
    "  \"Event-Driven Architecture\": \"score option\",\n",
    "  \"Microkernel Architecture\": \"score option\",\n",
    "  \"Microservices Architecture\": \"score option\",\n",
    "  \"Space-Based Architecture\": \"score option\",\n",
    "  \"Pipeline Architecture\": \"score option\",\n",
    "  \"Client-Server Architecture\": \"score option\"\n",
    "}}\n",
    "\n",
    "{example_run_prompt}\n",
    "\n",
    "\"\"\"}\n",
    "messages.append(generator_system_message)\n",
    "\n",
    "\n",
    "messages.append({'role': 'user', 'content': f\"\"\"\n",
    "I will give you a list of categorized user stories and a description created for a software project titled {project_title}. Please analyze them in detail and give me the scoring for each architecture pattern.\n",
    "The final scores must be given in json format after the detailed reasoning for each architecture pattern:\n",
    "\n",
    "{{\n",
    "  \"Layered Architecture\": \"score option\",\n",
    "  \"Event-Driven Architecture\": \"score option\",\n",
    "  \"Microkernel Architecture\": \"score option\",\n",
    "  \"Microservices Architecture\": \"score option\",\n",
    "  \"Space-Based Architecture\": \"score option\",\n",
    "  \"Pipeline Architecture\": \"score option\",\n",
    "  \"Client-Server Architecture\": \"score option\"\n",
    "}}\n",
    "\n",
    "Project Title: {project_title}\n",
    "\n",
    "Categorized User Stories:\n",
    "\n",
    "{project_content}\n",
    "\n",
    "\"\"\"})\n",
    "start_time = time.time()\n",
    "response = client.chat.completions.create(model=model_name, messages=messages, temperature=temperature, max_completion_tokens=num_ctx)\n",
    "end_time = time.time()\n",
    "score_generation_duration = end_time - start_time\n",
    "\n",
    "message = response.choices[0].message\n",
    "print(message.content)\n",
    "messages.append(message)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "To determine the most suitable architecture pattern for the Open Spending Project, we need to analyze the user stories and their requirements. The project involves data publishing, consuming, and visualization, with a focus on transparency in local government spending. The architecture should support usability, integrability, security, manageability, modularity, performance, and ease of development.\n",
      "\n",
      "### Layered Architecture\n",
      "- **Reasoning**: Layered architecture is suitable for applications with distinct layers of functionality, such as presentation, business logic, and data access. The user stories indicate a need for data visualization, API access, and data management, which can be effectively separated into layers.\n",
      "- **Specific User Stories**: The need for data visualization (\"As a Data Consuming User, I want to visualize by default in treemap, bubble tree, map and pivot table...\") and API access (\"As an API User, I want to be able to get results from multiple datasets...\") suggests a clear separation of concerns.\n",
      "- **Score**: \"Well suited\"\n",
      "\n",
      "### Event-Driven Architecture\n",
      "- **Reasoning**: Event-driven architecture is beneficial for systems that require real-time processing and responsiveness to events. The project involves data updates and visualization, which could benefit from event-driven mechanisms.\n",
      "- **Specific User Stories**: The need for real-time updates (\"As a Data Publishing User, I want my dataset update automatically as the source file/files changes...\") and dynamic data visualization (\"As a Data Consuming User, I want data visualizations to be served from a dynamic, in-memory data store...\") aligns with event-driven architecture.\n",
      "- **Score**: \"Well suited\"\n",
      "\n",
      "### Microkernel Architecture\n",
      "- **Reasoning**: Microkernel architecture is suitable for applications that require extensibility and adaptability, with a core system and plug-in modules. The project requires modularity and extensibility, but the user stories do not strongly indicate a need for a microkernel approach.\n",
      "- **Specific User Stories**: The need for modularity (\"As a Data Publishing User, I want to be able to edit the model of data I have already imported...\") suggests some alignment, but not strongly enough.\n",
      "- **Score**: \"Partially suitable\"\n",
      "\n",
      "### Microservices Architecture\n",
      "- **Reasoning**: Microservices architecture is ideal for applications that require scalability, flexibility, and independent deployment of services. The project involves various functionalities like data import, visualization, and API access, which can be developed as independent services.\n",
      "- **Specific User Stories**: The need for scalability and independent functionalities (\"As a System Architect, I want to design the system to decouple data ingestion from processing...\") supports the use of microservices.\n",
      "- **Score**: \"Perfectly aligned\"\n",
      "\n",
      "### Space-Based Architecture\n",
      "- **Reasoning**: Space-based architecture is designed for high scalability and handling large volumes of data with distributed processing. The project requires efficient data processing and high availability.\n",
      "- **Specific User Stories**: The need for distributed processing and high availability (\"As a Platform Administrator, I want to deploy and monitor distributed processing nodes...\") aligns with space-based architecture.\n",
      "- **Score**: \"Well suited\"\n",
      "\n",
      "### Pipeline Architecture\n",
      "- **Reasoning**: Pipeline architecture is suitable for data processing applications where data flows through a series of processing stages. The project involves data import and transformation, which could benefit from a pipeline approach.\n",
      "- **Specific User Stories**: The need for data import and processing (\"As a Data Publishing User, I want the data import pipeline to be implemented in a highly scalable and distributed manner...\") suggests some alignment.\n",
      "- **Score**: \"Moderately applicable\"\n",
      "\n",
      "### Client-Server Architecture\n",
      "- **Reasoning**: Client-server architecture is a traditional model for applications with a clear separation between client and server roles. The project involves data access and visualization, which can be supported by a client-server model.\n",
      "- **Specific User Stories**: The need for data access and visualization (\"As a Data Consuming User, I want to be able to filter, sort and aggregate data...\") aligns with client-server architecture.\n",
      "- **Score**: \"Moderately applicable\"\n",
      "\n",
      "Based on the analysis, here are the final scores:\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"Layered Architecture\": \"Well suited\",\n",
      "  \"Event-Driven Architecture\": \"Well suited\",\n",
      "  \"Microkernel Architecture\": \"Partially suitable\",\n",
      "  \"Microservices Architecture\": \"Perfectly aligned\",\n",
      "  \"Space-Based Architecture\": \"Well suited\",\n",
      "  \"Pipeline Architecture\": \"Moderately applicable\",\n",
      "  \"Client-Server Architecture\": \"Moderately applicable\"\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Self-refinement Process"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "source": [
    "\n",
    "# Let's capture the model's initial answer in a variable:\n",
    "current_answer = message.content  # e.g. the text with reasoning + final scores\n",
    "evaluator_context = [] # this context will be used in evaluator messages\n",
    "eval_system_message = {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": f\"\"\"You are the Evaluator. \n",
    "Your job: Go over the user stories thoroughly and evaluate the given reasoning and scores for each architecture pattern and provide score refinement suggestions if you are confident that the current score for any architecture pattern should be adjusted upward or downward.\n",
    "Mention all specific user stories that are relevant to your refinement suggestions.\n",
    "\n",
    "Here are the architecture patterns you will score:\n",
    "*Layered Architecture \n",
    "*Event-Driven Architecture\n",
    "*Microkernel Architecture \n",
    "*Microservices Architecture \n",
    "*Space-Based Architecture\n",
    "*Pipeline Architecture\n",
    "*Client-Server Architecture\n",
    "\n",
    "{pattern_descriptions_text}\n",
    "\n",
    "Here are the score options for the applicability of a pattern, ordered from lowest to highest:\n",
    "- \"Completely unsuitable\"\n",
    "- \"Partially suitable\"\n",
    "- \"Moderately applicable\"\n",
    "- \"Well suited\"\n",
    "- \"Perfectly aligned\"\n",
    "\n",
    "- Do not assume any new information about the project. The description and the user stories for the project remain the same. Do not suggest refinements for possible future changes.\n",
    "- Also, review your previous evaluations to ensure you don’t repeat the same refinement suggestion for an architecture pattern for the same reason.\n",
    "\n",
    "After your refinement suggestions, on a NEW line, end your response with exactly one of the two markers:\n",
    "- REFINE (if refinements are needed)\n",
    "- NO_REFINEMENT (if no refinements are needed)\n",
    "\n",
    "Do not include any additional text after that marker.\n",
    "Do not wrap it in quotes.\n",
    "\n",
    "Important:\n",
    "- Score can only be assigned one of the integer values that are given as the score options, no float values.\n",
    "- If you provide refinement for any of the pattern scores, do not use NO_REFINEMENT.\n",
    "Because this will stop the whole process and the current scores will be left unrefined.\n",
    "- Use NO_REFINEMENT only if you have zero refinements to suggest.\n",
    "\"\"\"\n",
    "}\n",
    "evaluator_context.append(eval_system_message)\n",
    "\n",
    "refiner_context = [] # this context will be used in refiner messages\n",
    "\n",
    "refine_system_message = {\n",
    "    \"role\": \"system\",\n",
    "    \"content\": f\"\"\"You are the Refiner. \n",
    "You will be given the user stories and the latest reasoning/scores for the project and the evaluator's feedback for the latest reasoning/scores.\n",
    "Make changes to the current scores according to the provided refinement suggestions.\n",
    "- Be as objective as possible during your scoring. The decisions need to be deterministic and reproducible.\n",
    "- Do not assume any new information about the project. The description and the user stories for the project remain the same.\n",
    "- IMPORTANT:  ONLY make a change for a score when the refinement suggestion for that score is reasonable and the assigned score must change. Otherwise keep it unchanged.\n",
    "\n",
    "Here are the architecture patterns you will score:\n",
    "*Layered Architecture \n",
    "*Event-Driven Architecture\n",
    "*Microkernel Architecture \n",
    "*Microservices Architecture \n",
    "*Space-Based Architecture\n",
    "*Pipeline Architecture\n",
    "*Client-Server Architecture\n",
    "\n",
    "{pattern_descriptions_text}\n",
    "\n",
    "Here are the score options for the applicability of a pattern, ordered from lowest to highest:\n",
    "- \"Completely unsuitable\"\n",
    "- \"Partially suitable\"\n",
    "- \"Moderately applicable\"\n",
    "- \"Well suited\"\n",
    "- \"Perfectly aligned\"\n",
    "\n",
    "IMPORTANT: When returning the refined scores, keep the same format in the updated response (reasoning and then the updated scores in json format).\n",
    "\n",
    "\"\"\"\n",
    "    }\n",
    "refiner_context.append(refine_system_message)\n",
    "# We'll define how many refinement loops to allow:\n",
    "MAX_REFINEMENT_ITERATIONS = 3\n",
    "\n",
    "start_time = time.time()\n",
    "refinement_iterations = MAX_REFINEMENT_ITERATIONS # if not set again in the loop, it means the model went through max iterations\n",
    "for i in range(MAX_REFINEMENT_ITERATIONS):\n",
    "    # 1) Evaluate the current answer\n",
    "    \n",
    "    evaluate_prompt = {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"\"\"\n",
    "            Evaluate the current reasoning and scores by taking the original categorized user stories into account and make refinement suggestions for the current answer:\n",
    "\n",
    "            Project Title: {project_title}\n",
    "\n",
    "            Categorized User Stories:\n",
    "\n",
    "            {project_content}\n",
    "\n",
    "            Current Answer:\n",
    "            {current_answer}\n",
    "\n",
    "            Refinement Suggestions:\n",
    "            \"\"\"\n",
    "    }\n",
    "    evaluator_context.append(evaluate_prompt)\n",
    "    eval_response = client.chat.completions.create(model=model_name, messages=evaluator_context, temperature=temperature, max_completion_tokens=num_ctx)\n",
    "    eval_message = eval_response.choices[0].message\n",
    "    eval_feedback = eval_message.content.strip()\n",
    "    evaluator_context.append(eval_message)\n",
    "    \n",
    "    print(f\"\\n=== Evaluator Feedback (Iteration {i+1}) ===\\n{eval_feedback}\\n\")\n",
    "    \n",
    "    # 2) If the evaluator indicates no changes are needed, break out\n",
    "    if \"NO_REFINEMENT\" in eval_feedback:\n",
    "        print(\"Evaluator says no changes are needed. Stopping refinement.\")\n",
    "        refinement_iterations = i\n",
    "        break\n",
    "    \n",
    "    # 3) Otherwise, refine\n",
    "    refine_prompt = {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"\"\"\n",
    "            Analyze the previous answer and the feedback of the evaluator, and then refine your previous answer by taking the feedback of the evaluator into account. \n",
    "            Keep the same format (reasoning and then the final scores in json format).\n",
    "            \n",
    "            Project Title: {project_title}\n",
    "\n",
    "            Categorized User Stories:\n",
    "\n",
    "            {project_content}\n",
    "            \n",
    "            Previous Answer:\n",
    "            {current_answer}\n",
    "\n",
    "            Evaluator Feedback:\n",
    "            {eval_feedback}\n",
    "\n",
    "            Revised Answer:\n",
    "            \"\"\"\n",
    "    }\n",
    "    refiner_context.append(refine_prompt)\n",
    "    \n",
    "    refiner_response = client.chat.completions.create(model=model_name, messages=refiner_context, temperature=temperature, max_completion_tokens=num_ctx)\n",
    "    refine_message = refiner_response.choices[0].message\n",
    "    revised_answer = refine_message.content.strip()\n",
    "    refiner_context.append(refine_message)\n",
    "    \n",
    "    print(f\"=== Refined Answer (Iteration {i+1}) ===\\n{revised_answer}\\n\")\n",
    "    \n",
    "    # Update current_answer for potential next iteration\n",
    "    current_answer = revised_answer\n",
    "\n",
    "end_time = time.time()\n",
    "refinement_duration = end_time - start_time\n",
    "# After the loop, current_answer holds the final refined output:\n",
    "print(\"=== Final Refined Scores & Reasoning ===\\n\", current_answer)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "=== Evaluator Feedback (Iteration 1) ===\n",
      "### Layered Architecture\n",
      "- **Current Score**: \"Well suited\"\n",
      "- **Evaluation**: The reasoning and user stories support the current score. The separation of concerns in data visualization and API access aligns well with a layered approach.\n",
      "- **Refinement Suggestion**: No change needed.\n",
      "\n",
      "### Event-Driven Architecture\n",
      "- **Current Score**: \"Well suited\"\n",
      "- **Evaluation**: The user stories related to real-time updates and dynamic data visualization support the use of event-driven architecture. The current score is appropriate.\n",
      "- **Refinement Suggestion**: No change needed.\n",
      "\n",
      "### Microkernel Architecture\n",
      "- **Current Score**: \"Partially suitable\"\n",
      "- **Evaluation**: The reasoning correctly identifies a lack of strong alignment with the microkernel approach. The user stories do not emphasize the need for a core system with plug-in modules.\n",
      "- **Refinement Suggestion**: No change needed.\n",
      "\n",
      "### Microservices Architecture\n",
      "- **Current Score**: \"Perfectly aligned\"\n",
      "- **Evaluation**: The user stories emphasize scalability, flexibility, and independent functionalities, which are well suited for microservices. The current score is justified.\n",
      "- **Refinement Suggestion**: No change needed.\n",
      "\n",
      "### Space-Based Architecture\n",
      "- **Current Score**: \"Well suited\"\n",
      "- **Evaluation**: The need for distributed processing and high availability aligns with space-based architecture. The current score is appropriate.\n",
      "- **Refinement Suggestion**: No change needed.\n",
      "\n",
      "### Pipeline Architecture\n",
      "- **Current Score**: \"Moderately applicable\"\n",
      "- **Evaluation**: The user stories related to data import and processing suggest a pipeline approach could be beneficial. However, the current score reflects the level of alignment accurately.\n",
      "- **Refinement Suggestion**: No change needed.\n",
      "\n",
      "### Client-Server Architecture\n",
      "- **Current Score**: \"Moderately applicable\"\n",
      "- **Evaluation**: The user stories related to data access and visualization support the use of a client-server model. The current score is reasonable.\n",
      "- **Refinement Suggestion**: No change needed.\n",
      "\n",
      "NO_REFINEMENT\n",
      "\n",
      "Evaluator says no changes are needed. Stopping refinement.\n",
      "=== Final Refined Scores & Reasoning ===\n",
      " To determine the most suitable architecture pattern for the Open Spending Project, we need to analyze the user stories and their requirements. The project involves data publishing, consuming, and visualization, with a focus on transparency in local government spending. The architecture should support usability, integrability, security, manageability, modularity, performance, and ease of development.\n",
      "\n",
      "### Layered Architecture\n",
      "- **Reasoning**: Layered architecture is suitable for applications with distinct layers of functionality, such as presentation, business logic, and data access. The user stories indicate a need for data visualization, API access, and data management, which can be effectively separated into layers.\n",
      "- **Specific User Stories**: The need for data visualization (\"As a Data Consuming User, I want to visualize by default in treemap, bubble tree, map and pivot table...\") and API access (\"As an API User, I want to be able to get results from multiple datasets...\") suggests a clear separation of concerns.\n",
      "- **Score**: \"Well suited\"\n",
      "\n",
      "### Event-Driven Architecture\n",
      "- **Reasoning**: Event-driven architecture is beneficial for systems that require real-time processing and responsiveness to events. The project involves data updates and visualization, which could benefit from event-driven mechanisms.\n",
      "- **Specific User Stories**: The need for real-time updates (\"As a Data Publishing User, I want my dataset update automatically as the source file/files changes...\") and dynamic data visualization (\"As a Data Consuming User, I want data visualizations to be served from a dynamic, in-memory data store...\") aligns with event-driven architecture.\n",
      "- **Score**: \"Well suited\"\n",
      "\n",
      "### Microkernel Architecture\n",
      "- **Reasoning**: Microkernel architecture is suitable for applications that require extensibility and adaptability, with a core system and plug-in modules. The project requires modularity and extensibility, but the user stories do not strongly indicate a need for a microkernel approach.\n",
      "- **Specific User Stories**: The need for modularity (\"As a Data Publishing User, I want to be able to edit the model of data I have already imported...\") suggests some alignment, but not strongly enough.\n",
      "- **Score**: \"Partially suitable\"\n",
      "\n",
      "### Microservices Architecture\n",
      "- **Reasoning**: Microservices architecture is ideal for applications that require scalability, flexibility, and independent deployment of services. The project involves various functionalities like data import, visualization, and API access, which can be developed as independent services.\n",
      "- **Specific User Stories**: The need for scalability and independent functionalities (\"As a System Architect, I want to design the system to decouple data ingestion from processing...\") supports the use of microservices.\n",
      "- **Score**: \"Perfectly aligned\"\n",
      "\n",
      "### Space-Based Architecture\n",
      "- **Reasoning**: Space-based architecture is designed for high scalability and handling large volumes of data with distributed processing. The project requires efficient data processing and high availability.\n",
      "- **Specific User Stories**: The need for distributed processing and high availability (\"As a Platform Administrator, I want to deploy and monitor distributed processing nodes...\") aligns with space-based architecture.\n",
      "- **Score**: \"Well suited\"\n",
      "\n",
      "### Pipeline Architecture\n",
      "- **Reasoning**: Pipeline architecture is suitable for data processing applications where data flows through a series of processing stages. The project involves data import and transformation, which could benefit from a pipeline approach.\n",
      "- **Specific User Stories**: The need for data import and processing (\"As a Data Publishing User, I want the data import pipeline to be implemented in a highly scalable and distributed manner...\") suggests some alignment.\n",
      "- **Score**: \"Moderately applicable\"\n",
      "\n",
      "### Client-Server Architecture\n",
      "- **Reasoning**: Client-server architecture is a traditional model for applications with a clear separation between client and server roles. The project involves data access and visualization, which can be supported by a client-server model.\n",
      "- **Specific User Stories**: The need for data access and visualization (\"As a Data Consuming User, I want to be able to filter, sort and aggregate data...\") aligns with client-server architecture.\n",
      "- **Score**: \"Moderately applicable\"\n",
      "\n",
      "Based on the analysis, here are the final scores:\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"Layered Architecture\": \"Well suited\",\n",
      "  \"Event-Driven Architecture\": \"Well suited\",\n",
      "  \"Microkernel Architecture\": \"Partially suitable\",\n",
      "  \"Microservices Architecture\": \"Perfectly aligned\",\n",
      "  \"Space-Based Architecture\": \"Well suited\",\n",
      "  \"Pipeline Architecture\": \"Moderately applicable\",\n",
      "  \"Client-Server Architecture\": \"Moderately applicable\"\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Summarize the evaluation result"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "source": [
    "refiner_context.append({'role': 'system', 'content': \"\"\"\n",
    "Ok now the evaluation process is finished. take the first assessment and the last assessment from the user. \n",
    "And just return the final scores before and after the evaluation-refinement process\n",
    "following this json format below:\n",
    "\n",
    "{\n",
    "  \"Layered Architecture\": {\"before\": \"score option\", \"after\": \"score option\"},\n",
    "  \"Event-Driven Architecture\": {\"before\": \"score option\", \"after\": \"score option\"},\n",
    "  \"Microkernel Architecture\": {\"before\": \"score option\", \"after\": \"score option\"},\n",
    "  \"Microservices Architecture\": {\"before\": \"score option\", \"after\": \"score option\"},\n",
    "  \"Space-Based Architecture\": {\"before\": \"score option\", \"after\": \"score option\"},\n",
    "  \"Pipeline Architecture\": {\"before\": \"score option\", \"after\": \"score option\"},\n",
    "  \"Client-Server Architecture\": {\"before\": \"score option\", \"after\": \"score option\"}\n",
    "}\n",
    "\"\"\"})\n",
    "\n",
    "refiner_context.append({'role': 'user', 'content': f\"\"\"\n",
    "read the first assessment and the last assessment, and return the final scores before and after the evaluation-refinement process\n",
    "following this json format below:\n",
    "\n",
    "{{\n",
    "  \"Layered Architecture\": {{\"before\": \"score option\", \"after\": \"score option\"}},\n",
    "  \"Event-Driven Architecture\": {{\"before\": \"score option\", \"after\": \"score option\"}},\n",
    "  \"Microkernel Architecture\": {{\"before\": \"score option\", \"after\": \"score option\"}},\n",
    "  \"Microservices Architecture\": {{\"before\": \"score option\", \"after\": \"score option\"}},\n",
    "  \"Space-Based Architecture\": {{\"before\": \"score option\", \"after\": \"score option\"}},\n",
    "  \"Pipeline Architecture\": {{\"before\": \"score option\", \"after\": \"score option\"}},\n",
    "  \"Client-Server Architecture\": {{\"before\": \"score option\", \"after\": \"score option\"}}\n",
    "}}\n",
    "\n",
    "\n",
    "first assessment:\n",
    "{messages[2].content.strip()}\n",
    "\n",
    "last assessment:\n",
    "{current_answer.strip()}\n",
    "\"\"\"})\n",
    "response = client.chat.completions.create(model=model_name, \n",
    "    messages=refiner_context, temperature=temperature, max_completion_tokens=num_ctx) # need deterministic answer\n",
    "message = response.choices[0].message\n",
    "refiner_context.append(message)\n",
    "print(message.content)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "```json\n",
      "{\n",
      "  \"Layered Architecture\": {\"before\": \"Well suited\", \"after\": \"Well suited\"},\n",
      "  \"Event-Driven Architecture\": {\"before\": \"Well suited\", \"after\": \"Well suited\"},\n",
      "  \"Microkernel Architecture\": {\"before\": \"Partially suitable\", \"after\": \"Partially suitable\"},\n",
      "  \"Microservices Architecture\": {\"before\": \"Perfectly aligned\", \"after\": \"Perfectly aligned\"},\n",
      "  \"Space-Based Architecture\": {\"before\": \"Well suited\", \"after\": \"Well suited\"},\n",
      "  \"Pipeline Architecture\": {\"before\": \"Moderately applicable\", \"after\": \"Moderately applicable\"},\n",
      "  \"Client-Server Architecture\": {\"before\": \"Moderately applicable\", \"after\": \"Moderately applicable\"}\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "source": [
    "import json\n",
    "\n",
    "def parse_json_in_string(s):\n",
    "    \"\"\"\n",
    "    Parses and returns the first JSON object found in the input string.\n",
    "\n",
    "    Args:\n",
    "        s (str): The input string that contains at least one JSON object.\n",
    "\n",
    "    Returns:\n",
    "        object: The Python representation of the parsed JSON object.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If no JSON object is found or if decoding fails.\n",
    "    \"\"\"\n",
    "    # Find the first occurrence of '{'\n",
    "    start = s.find('{')\n",
    "    if start == -1:\n",
    "         return None\n",
    "\n",
    "    decoder = json.JSONDecoder()\n",
    "    try:\n",
    "        obj, _ = decoder.raw_decode(s, idx=start)\n",
    "        return obj\n",
    "    except json.JSONDecodeError as e:\n",
    "        return None"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Calculate weighted Kappa score"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "source": [
    "from sklearn.metrics import cohen_kappa_score\n",
    "architecture_patterns = [\"Layered Architecture\", \"Event-Driven Architecture\",\n",
    " \"Microkernel Architecture\", \"Microservices Architecture\", \"Space-Based Architecture\", \"Pipeline Architecture\", \"Client-Server Architecture\"]\n",
    "\n",
    "refiner_context = [message if isinstance(message, dict) else message.dict() for message in refiner_context]\n",
    "\n",
    "scores = {\"Completely unsuitable\": 1, \"Partially suitable\": 2, \"Moderately applicable\": 3, \"Well suited\": 4, \"Perfectly aligned\": 5}\n",
    "with open('expert_answers.json') as f:\n",
    "    expert_answers = json.load(f)\n",
    "llm_answers = parse_json_in_string(refiner_context[-1][\"content\"])\n",
    "answers_before = [scores[llm_answers[pattern][\"before\"]] for pattern in architecture_patterns]\n",
    "answers_after = [scores[llm_answers[pattern][\"after\"]] for pattern in architecture_patterns]\n",
    "score_before_refinement = cohen_kappa_score(expert_answers[selected_project], answers_before, labels=[1, 2, 3, 4, 5], weights='quadratic')\n",
    "score_after_refinement = cohen_kappa_score(expert_answers[selected_project], answers_after, labels=[1, 2, 3, 4, 5], weights='quadratic')\n",
    "\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/var/folders/r9/r79c48353r5cvd6d3tz920_h0000gn/T/ipykernel_59160/564283655.py:5: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
      "  refiner_context = [message if isinstance(message, dict) else message.dict() for message in refiner_context]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Print log to output file"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "source": [
    "from datetime import datetime\n",
    "import json\n",
    "from pathlib import Path\n",
    "Path(\"./logs/self-refinement-one-shot\").mkdir(parents=True, exist_ok=True)\n",
    "Path(\"./logs/self-refinement-zero-shot\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "final_data = {\n",
    "        \"modelName\": model_name,\n",
    "        \"temperature\": temperature,\n",
    "        \"context_limit\": num_ctx,\n",
    "        \"projectTitle\": project_title,\n",
    "        \"file_name\": file_name,\n",
    "        \"selfRefinement\": True,\n",
    "        \"oneShot\": one_shot,\n",
    "        \"wckScoreBeforeRefinement\": score_before_refinement,\n",
    "        \"wckScoreAfterRefinement\": score_after_refinement,\n",
    "        \"patternDescriptionsAdded\": add_pattern_descriptions,\n",
    "        \"timestamp\": datetime.now().strftime(\"%Y%m%d_%H%M%S\"),\n",
    "        \"json_output_parsed\": parse_json_in_string(refiner_context[-1][\"content\"]),\n",
    "        \"numberOfIterations\": refinement_iterations,\n",
    "        \"maxIterationsAllowed\": MAX_REFINEMENT_ITERATIONS,\n",
    "        \"messages\": [message[\"content\"] for message in refiner_context],\n",
    "        \"scoreGenerationDuration\": score_generation_duration,\n",
    "        \"refinementDuration\": refinement_duration\n",
    "    }\n",
    "    \n",
    "# 3) Generate a filename based on model name and current timestamp\n",
    "if one_shot:\n",
    "    filename = f\"./logs/self-refinement-one-shot/log_{model_version}_{final_data['timestamp']}.json\"\n",
    "else:\n",
    "    filename = f\"./logs/self-refinement-zero-shot/log_{model_version}_{final_data['timestamp']}.json\"\n",
    "# 4) Write the conversation to a JSON file\n",
    "with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(final_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"Assesment complete. The whole conversation is saved to {filename}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Assesment complete. The whole conversation is saved to ./logs/self-refinement-zero-shot/log_gpt-4o_20250205_160646.json\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.10 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "interpreter": {
   "hash": "95ec9ec1504d83f612128e0fb229072f90bbb4cb09d9d5d93b5dd26e0ca2cfd1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}