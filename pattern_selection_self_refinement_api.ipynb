{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 329,
   "source": [
    "pip install groq scikit-learn openai"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: groq in /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages (0.17.0)\n",
      "Requirement already satisfied: scikit-learn in /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages (1.6.1)\n",
      "Requirement already satisfied: openai in /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages (1.61.0)\n",
      "Requirement already satisfied: sniffio in /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages (from groq) (1.3.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages (from groq) (4.3.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages (from groq) (1.9.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.10 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages (from groq) (4.12.2)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages (from groq) (2.10.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages (from groq) (0.27.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages (from scikit-learn) (1.13.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: tqdm>4 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages (from openai) (4.63.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages (from openai) (0.8.2)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages (from anyio<5,>=3.5.0->groq) (1.2.0)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages (from anyio<5,>=3.5.0->groq) (2.10)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages (from httpx<1,>=0.23.0->groq) (1.0.5)\n",
      "Requirement already satisfied: certifi in /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages (from httpx<1,>=0.23.0->groq) (2024.2.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages (from pydantic<3,>=1.9.0->groq) (2.27.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-28T15:02:44.149935Z",
     "iopub.status.busy": "2024-11-28T15:02:44.149036Z",
     "iopub.status.idle": "2024-11-28T15:02:46.988655Z",
     "shell.execute_reply": "2024-11-28T15:02:46.987904Z",
     "shell.execute_reply.started": "2024-11-28T15:02:44.149875Z"
    },
    "trusted": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load input file"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "source": [
    "import os\n",
    "\n",
    "project_title_map = {\n",
    "        \"archives_space\": \"Archives Space Project\",\n",
    "        \"archives_space_old\": \"Archives Space Project\",\n",
    "        \"neurohub\": \"NeuroHub Project\",\n",
    "        \"open_spending\": \"Open Spending Project\",\n",
    "        \"open_spending_old\": \"Open Spending Project\",\n",
    "        \"planning_poker\": \"Planning Poker Project\",\n",
    "        \"recycling\": \"Recycling Project\",\n",
    "        \"color_ide\": \"ColorIDE Project\"\n",
    "    }\n",
    "projects = [\"archives_space\", \"neurohub\", \"open_spending\", \"planning_poker\", \"recycling\", \"color_ide\"]\n",
    "\n",
    "selected_project = os.environ.get(\"project\")\n",
    "if selected_project is None:\n",
    "    selected_project = projects[2]\n",
    "file_name = \"user_stories_{}.json\".format(selected_project)\n",
    "project_path = \"./input_files/{}\".format(file_name)\n",
    "with open(project_path, 'r') as file:\n",
    "        project_content = file.read()\n",
    "        project_title = project_title_map[selected_project]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load model and configure model parameters"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "source": [
    "import groq\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=\"\", base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\")\n",
    "model_version = \"gemini-2.0-flash\"\n",
    "\n",
    "\n",
    "#client = groq.Client(api_key=\"\")\n",
    "#model_version = \"llama-3.3-70b-versatile\"\n",
    "\n",
    "#client = OpenAI(api_key=\"\")\n",
    "#model_version = \"gpt-4o\"\n",
    "\n",
    "#model_version = \"llama3.1:8b-instruct-fp16\"\n",
    "\n",
    "num_ctx = 15000 # context length is higher because of the refinement process\n",
    "temperature = 0.000000001  # should be kept 0 for deterministic results, default value 0.8\n",
    "#temperature = 0\n",
    "top_p = 0.0000001\n",
    "model_name = model_version\n",
    "\n",
    "one_shot = False\n",
    "\n",
    "add_pattern_descriptions = True"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load one-shot example if set True"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "source": [
    "import json\n",
    "if one_shot:\n",
    "    with open('example_reasoning_1.txt', 'r') as file:\n",
    "        example_run_text = file.read()\n",
    "        example_run_prompt = f\"\"\"\n",
    "        I will give you an example run for another software project just to show you the reasoning process and the output format. \n",
    "        VERY IMPORTANT NOTE: \"NEVER COPY THE REASONING GIVEN IN THE EXAMPLE RUN! This Example is for Reference Only, Come Up With Your Own Reasoning for The User Input!\"\n",
    "        - EXAMPLE RUN START - \n",
    "\n",
    "        {example_run_text}\n",
    "        \n",
    "        - EXAMPLE RUN END -\n",
    "\n",
    "        \"\"\"\n",
    "else:\n",
    "    example_run_prompt = \"\"\n",
    "\n",
    "if add_pattern_descriptions:\n",
    "    with open('pattern_descriptions.txt', 'r') as file:\n",
    "        pattern_descriptions_text = file.read()\n",
    "else:\n",
    "    pattern_descriptions_text = \"\"\n",
    "pattern_descriptions_text\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'Architecture Pattern Descriptions:\\n\\nLayered Architecture: \\nLayered Architecture organizes the system into a set of layers, where each layer has a distinct responsibility and communicates only with its adjacent layers. This separation of concerns simplifies maintenance and testing.\\nEvent-Driven Architecture: \\nEvent-Driven Architecture centers on the production, detection, and consumption of events, enabling asynchronous communication among decoupled components.\\nMicrokernel Architecture: \\nMicrokernel Architecture features a minimal core system that handles essential services, while additional functionalities are provided through plug-in modules.\\nMicroservices Architecture: \\nMicroservices Architecture decomposes an application into a suite of small, independently deployable services, each responsible for a specific business function.\\nSpace-Based Architecture: \\nThe Space-Based Architecture pattern (also known as Cloud Architecture pattern) tackles scalability challenges by distributing application data and processing across multiple independent \"spaces.\" Each space contains a portion of the data and the logic to process it, minimizing bottlenecks and enabling horizontal scaling. Data is often replicated across spaces for fault tolerance and performance. A central coordinating mechanism manages these spaces, allowing them to dynamically scale up or down based on demand. This approach, commonly used in cloud environments, reduces the reliance on a central database constraint, enabling high scalability. This pattern is particularly effective for applications with variable and unpredictable user volumes, as it can dynamically adjust resources to meet demand, minimizing factors that typically limit scaling.\\nPipeline Architecture: \\nPipeline Architecture organizes the system into a series of processing stages (filters) connected by channels (pipes). Data flows through the pipeline, being transformed at each stage. It simplifies complex processing tasks by breaking them into smaller, independent steps.\\nClient-Server Architecture: \\nClient-Server Architecture separates the application into client components that request services and server components that provide services. This pattern is common in web applications, where the browser acts as the client and a web server acts as the server.'"
      ]
     },
     "metadata": {},
     "execution_count": 332
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Get first assessment from model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "source": [
    "\n",
    "messages = []\n",
    "import time\n",
    "\n",
    "\n",
    "generator_system_message = {'role': 'system', 'content': f\"\"\"\n",
    "You are a software architect. Your task is to get a list of categorized user stories with a description, analyze them in detail and\n",
    "assign a score for each architecture pattern depending on their relevance to the project and \n",
    "if it would prove useful in the implementation of the described project.\n",
    "- Go over all of the user stories thoroughly, think step by step and explain your though process clearly. \n",
    "- Mention all specific user stories that helped you to make a decision.\n",
    "- Be as objective as possible during your scoring. The decisions need to be deterministic and reproducible.\n",
    "\n",
    "Here are the architecture patterns you will score:\n",
    "*Layered Architecture \n",
    "*Event-Driven Architecture\n",
    "*Microkernel Architecture \n",
    "*Microservices Architecture \n",
    "*Space-Based Architecture\n",
    "*Pipeline Architecture\n",
    "*Client-Server Architecture\n",
    "\n",
    "{pattern_descriptions_text}\n",
    "\n",
    "Here are the score options for the applicability of a pattern, ordered from lowest to highest:\n",
    "- \"Completely unsuitable\"\n",
    "- \"Partially suitable\"\n",
    "- \"Moderately applicable\"\n",
    "- \"Well suited\"\n",
    "- \"Perfectly aligned\"\n",
    "\n",
    "IMPORTANT: First explain your reasoning for each pattern, mention the relevant user stories and then print the final scores in this json format:\n",
    "\n",
    "{{\n",
    "  \"Layered Architecture\": \"score option\",\n",
    "  \"Event-Driven Architecture\": \"score option\",\n",
    "  \"Microkernel Architecture\": \"score option\",\n",
    "  \"Microservices Architecture\": \"score option\",\n",
    "  \"Space-Based Architecture\": \"score option\",\n",
    "  \"Pipeline Architecture\": \"score option\",\n",
    "  \"Client-Server Architecture\": \"score option\"\n",
    "}}\n",
    "\n",
    "{example_run_prompt}\n",
    "\n",
    "\"\"\"}\n",
    "messages.append(generator_system_message)\n",
    "\n",
    "\n",
    "messages.append({'role': 'user', 'content': f\"\"\"\n",
    "I will give you a list of categorized user stories and a description created for a software project titled {project_title}. Please analyze them in detail and give me the scoring for each architecture pattern.\n",
    "The final scores must be given in json format after the detailed reasoning for each architecture pattern:\n",
    "\n",
    "{{\n",
    "  \"Layered Architecture\": \"score option\",\n",
    "  \"Event-Driven Architecture\": \"score option\",\n",
    "  \"Microkernel Architecture\": \"score option\",\n",
    "  \"Microservices Architecture\": \"score option\",\n",
    "  \"Space-Based Architecture\": \"score option\",\n",
    "  \"Pipeline Architecture\": \"score option\",\n",
    "  \"Client-Server Architecture\": \"score option\"\n",
    "}}\n",
    "\n",
    "Project Title: {project_title}\n",
    "\n",
    "Categorized User Stories:\n",
    "\n",
    "{project_content}\n",
    "\n",
    "\"\"\"})\n",
    "start_time = time.time()\n",
    "response = client.chat.completions.create(model=model_name, messages=messages, top_p=top_p, temperature=temperature, max_completion_tokens=num_ctx)\n",
    "end_time = time.time()\n",
    "score_generation_duration = end_time - start_time\n",
    "\n",
    "message = response.choices[0].message\n",
    "print(message.content)\n",
    "messages.append(message)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Okay, I will analyze the user stories for the Open Spending Project and provide a relevance score for each architecture pattern, along with detailed reasoning.\n",
      "\n",
      "**Layered Architecture:**\n",
      "\n",
      "Reasoning: The Open Spending project can benefit from a layered architecture. The application has distinct layers such as the presentation layer (user interface for data visualization and interaction), the business logic layer (data processing, aggregation, currency conversion, and API handling), and the data persistence layer (storage of datasets and metadata). User stories like \"As a Data Consuming User, I want to be able to filter, sort and aggregate data by multiple dimensions and measures\" and \"As an API User, I want to be able to get a set of monetary measures transferred to different currencies\" highlight the need for a business logic layer. The separation of concerns offered by a layered architecture will improve maintainability, testability, and scalability.\n",
      "\n",
      "**Event-Driven Architecture:**\n",
      "\n",
      "Reasoning: While not explicitly stated, an event-driven architecture could be useful for certain aspects of the Open Spending project. For example, data updates or uploads could trigger events that initiate data processing, indexing, or notification processes. The user story \"As a Data Publishing User, I want to have my dataset update automatically as the source file/files changes\" could be implemented using an event-driven approach. However, the core functionality doesn't heavily rely on asynchronous events, so it's not a primary architectural driver.\n",
      "\n",
      "**Microkernel Architecture:**\n",
      "\n",
      "Reasoning: The Open Spending project doesn't seem to fit the microkernel architecture pattern very well. The core functionalities, such as data import, processing, visualization, and API access, are relatively well-defined and integrated. There isn't a clear need to separate a minimal core system from optional plug-in modules. While some features like specific data import formats or visualization types could potentially be implemented as plugins, it's not a central requirement.\n",
      "\n",
      "**Microservices Architecture:**\n",
      "\n",
      "Reasoning: The Open Spending project could potentially benefit from a microservices architecture. Different functionalities, such as data ingestion, data processing/normalization, API, visualization, user management, and search, could be implemented as independent microservices. This would allow for independent deployment, scaling, and fault isolation. The user stories related to API access (\"As an API User, I want to be able to make queries over millions of rows in seconds\") and data processing (\"As a Data Consuming User, I need visualizations to be created quickly, even with GBs of data\") suggest that independent scaling of these components could be beneficial.\n",
      "\n",
      "**Space-Based Architecture:**\n",
      "\n",
      "Reasoning: The user stories \"As an API User, I want to have consistently high download/upload speed\", \"As an API User, I want the API service to be available 24/7\", \"As a Data Consuming User, I need visualizations to be created quickly, even with GBs of data.\" and \"As an API User, I want to be able to make queries over millions of rows in seconds\" suggest that the project needs to handle large datasets and high traffic. Space-based architecture is designed to handle these types of requirements.\n",
      "\n",
      "**Pipeline Architecture:**\n",
      "\n",
      "Reasoning: A pipeline architecture could be suitable for the data ingestion and processing aspects of the Open Spending project. Data could flow through a pipeline of stages, such as data validation, cleansing, transformation, and indexing. The user stories related to data import (\"As a Data Publishing User, I want to be able to import data in Excel, JSON, Google Spreadsheet\") and data validation (\"As a Data Publishing User, I want to know if my CSV file is valid\") support this idea.\n",
      "\n",
      "**Client-Server Architecture:**\n",
      "\n",
      "Reasoning: The Open Spending project is inherently a client-server application. Users interact with the system through a web browser (client), and the server handles requests, data processing, and storage. The API also follows a client-server model. This pattern is fundamental to the project's architecture.\n",
      "\n",
      "Final Scores in JSON format:\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"Layered Architecture\": \"Well suited\",\n",
      "  \"Event-Driven Architecture\": \"Partially suitable\",\n",
      "  \"Microkernel Architecture\": \"Completely unsuitable\",\n",
      "  \"Microservices Architecture\": \"Well suited\",\n",
      "  \"Space-Based Architecture\": \"Moderately applicable\",\n",
      "  \"Pipeline Architecture\": \"Partially suitable\",\n",
      "  \"Client-Server Architecture\": \"Perfectly aligned\"\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Self-refinement Process"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "source": [
    "\n",
    "# Let's capture the model's initial answer in a variable:\n",
    "current_answer = message.content  # e.g. the text with reasoning + final scores\n",
    "evaluator_context = messages.copy() # this context will be used in evaluator messages\n",
    "eval_system_message = {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"\"\"\n",
    "Your task has changed. You are now the Evaluator.\n",
    "Your job is to thoroughly review the user stories, evaluate the provided reasoning and scores for each architectural pattern, and clearly recommend any necessary score adjustments—explicitly indicating whether they should be raised or lowered, as this distinction is critical. \n",
    "- Mention all specific user stories that are relevant to your refinement suggestions.\n",
    "\n",
    "Here are the score options for the applicability of a pattern, ordered from lowest to highest:\n",
    "- \"Completely unsuitable\"\n",
    "- \"Partially suitable\"\n",
    "- \"Moderately applicable\"\n",
    "- \"Well suited\"\n",
    "- \"Perfectly aligned\"\n",
    "\n",
    "- Do not assume any new information about the project. The description and the user stories for the project remain the same. Do not suggest refinements for possible future changes.\n",
    "- Only suggest a refinement when you are certain that the assigned score is incorrect and needs adjustment.\n",
    "- Also, review your previous evaluations to ensure you don’t repeat the same refinement suggestion for an architecture pattern for the same reason.\n",
    "\n",
    "After your refinement suggestions, on a NEW line, end your response with exactly one of the two markers:\n",
    "- REFINE (if refinements are needed)\n",
    "- NO_REFINEMENT (if no refinements are needed)\n",
    "\n",
    "Do not include any additional text after that marker.\n",
    "Do not wrap it in quotes.\n",
    "\n",
    "Important:\n",
    "- Score can only be assigned one of the integer values that are given as the score options, no float values.\n",
    "- If you provide refinement for any of the pattern scores, do not use NO_REFINEMENT.\n",
    "Because this will stop the whole process and the current scores will be left unrefined.\n",
    "- Use NO_REFINEMENT only if you have zero refinements to suggest.\n",
    "\"\"\"\n",
    "}\n",
    "evaluator_context.append(eval_system_message)\n",
    "\n",
    "refiner_context = messages.copy() # this context will be used in refiner messages\n",
    "\n",
    "refine_system_message = {\n",
    "    \"role\": \"system\",\n",
    "    \"content\": \"\"\"\n",
    "Your task has changed. You are now the Refiner. \n",
    "You will be given the latest reasoning/scores for the project and the evaluator's feedback for the latest reasoning/scores.\n",
    "Make changes to the current scores according to the provided refinement suggestions\n",
    "- Do not assume any new information about the project. The description and the user stories for the project remain the same.\n",
    "- IMPORTANT:  ONLY make a change for a score when the refinement suggestion for that score is reasonable and the assigned score must change. Otherwise keep it unchanged.\n",
    "\n",
    "When returning the refined scores, keep the same format in the updated response (reasoning and then the updated scores in json format).\n",
    "\n",
    "Here are the score options for the applicability of a pattern, ordered from lowest to highest:\n",
    "- \"Completely unsuitable\"\n",
    "- \"Partially suitable\"\n",
    "- \"Moderately applicable\"\n",
    "- \"Well suited\"\n",
    "- \"Perfectly aligned\"\n",
    "\n",
    "\"\"\"\n",
    "    }\n",
    "refiner_context.append(refine_system_message)\n",
    "# We'll define how many refinement loops to allow:\n",
    "MAX_REFINEMENT_ITERATIONS = 3\n",
    "\n",
    "start_time = time.time()\n",
    "refinement_iterations = MAX_REFINEMENT_ITERATIONS # if not set again in the loop, it means the model went through max iterations\n",
    "for i in range(MAX_REFINEMENT_ITERATIONS):\n",
    "    # 1) Evaluate the current answer\n",
    "    \n",
    "    evaluate_prompt = {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"\"\"\n",
    "            Evaluate the current reasoning and scores by taking the original categorized user stories into account and make refinement suggestions for the current answer:\n",
    "\n",
    "            Current Answer:\n",
    "            {current_answer}\n",
    "\n",
    "            Refinement Suggestions:\n",
    "            \"\"\"\n",
    "    }\n",
    "    evaluator_context.append(evaluate_prompt)\n",
    "    eval_response = client.chat.completions.create(model=model_name, \n",
    "    messages=evaluator_context, temperature=temperature, top_p=top_p, max_completion_tokens=num_ctx)\n",
    "    eval_message = eval_response.choices[0].message\n",
    "    eval_feedback = eval_message.content.strip()\n",
    "    evaluator_context.append(eval_message)\n",
    "    \n",
    "    print(f\"\\n=== Evaluator Feedback (Iteration {i+1}) ===\\n{eval_feedback}\\n\")\n",
    "    \n",
    "    # 2) If the evaluator indicates no changes are needed, break out\n",
    "    if \"NO_REFINEMENT\" in eval_feedback:\n",
    "        print(\"Evaluator says no changes are needed. Stopping refinement.\")\n",
    "        refinement_iterations = i\n",
    "        break\n",
    "    \n",
    "    # 3) Otherwise, refine\n",
    "    refine_prompt = {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"\"\"\n",
    "            Analyze the previous answer and the feedback of the evaluator, and then refine your previous answer by taking the feedback of the evaluator into account. \n",
    "            Keep the same format (reasoning and then the final scores in json format).\n",
    "\n",
    "            Previous Answer:\n",
    "            {current_answer}\n",
    "\n",
    "            Evaluator Feedback:\n",
    "            {eval_feedback}\n",
    "\n",
    "            Revised Answer:\n",
    "            \"\"\"\n",
    "    }\n",
    "    refiner_context.append(refine_prompt)\n",
    "    \n",
    "    refine_response = client.chat.completions.create(model=model_name, \n",
    "    messages=refiner_context, temperature=temperature, top_p=top_p, max_completion_tokens=num_ctx)\n",
    "    refine_message = refine_response.choices[0].message\n",
    "    revised_answer = refine_message.content.strip()\n",
    "    refiner_context.append(refine_message)\n",
    "    \n",
    "    print(f\"=== Refined Answer (Iteration {i+1}) ===\\n{revised_answer}\\n\")\n",
    "    \n",
    "    # Update current_answer for potential next iteration\n",
    "    current_answer = revised_answer\n",
    "\n",
    "end_time = time.time()\n",
    "refinement_duration = end_time - start_time\n",
    "# After the loop, current_answer holds the final refined output:\n",
    "print(\"=== Final Refined Scores & Reasoning ===\\n\", current_answer)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "=== Evaluator Feedback (Iteration 1) ===\n",
      "Okay, I have reviewed the reasoning and scores for each architectural pattern. Here are my refinement suggestions:\n",
      "\n",
      "**Space-Based Architecture:**\n",
      "\n",
      "Reasoning: The current score is \"Moderately applicable\". While the mentioned user stories do point to performance and availability needs, the space-based architecture is more than just sufficient. The project deals with large datasets and requires high availability and scalability, making space-based architecture a very good fit. The ability to distribute data and processing across multiple nodes to handle high traffic and large datasets aligns well with the project's requirements.\n",
      "\n",
      "Suggestion: Increase the score to \"Well suited\".\n",
      "\n",
      "REFINE\n",
      "\n",
      "=== Refined Answer (Iteration 1) ===\n",
      "Okay, I will refine the previous answer based on the evaluator's feedback, specifically increasing the score for Space-Based Architecture to \"Well suited\".\n",
      "\n",
      "**Layered Architecture:**\n",
      "\n",
      "Reasoning: The Open Spending project can benefit from a layered architecture. The application has distinct layers such as the presentation layer (user interface for data visualization and interaction), the business logic layer (data processing, aggregation, currency conversion, and API handling), and the data persistence layer (storage of datasets and metadata). User stories like \"As a Data Consuming User, I want to be able to filter, sort and aggregate data by multiple dimensions and measures\" and \"As an API User, I want to be able to get a set of monetary measures transferred to different currencies\" highlight the need for a business logic layer. The separation of concerns offered by a layered architecture will improve maintainability, testability, and scalability.\n",
      "\n",
      "**Event-Driven Architecture:**\n",
      "\n",
      "Reasoning: While not explicitly stated, an event-driven architecture could be useful for certain aspects of the Open Spending project. For example, data updates or uploads could trigger events that initiate data processing, indexing, or notification processes. The user story \"As a Data Publishing User, I want to have my dataset update automatically as the source file/files changes\" could be implemented using an event-driven approach. However, the core functionality doesn't heavily rely on asynchronous events, so it's not a primary architectural driver.\n",
      "\n",
      "**Microkernel Architecture:**\n",
      "\n",
      "Reasoning: The Open Spending project doesn't seem to fit the microkernel architecture pattern very well. The core functionalities, such as data import, processing, visualization, and API access, are relatively well-defined and integrated. There isn't a clear need to separate a minimal core system from optional plug-in modules. While some features like specific data import formats or visualization types could potentially be implemented as plugins, it's not a central requirement.\n",
      "\n",
      "**Microservices Architecture:**\n",
      "\n",
      "Reasoning: The Open Spending project could potentially benefit from a microservices architecture. Different functionalities, such as data ingestion, data processing/normalization, API, visualization, user management, and search, could be implemented as independent microservices. This would allow for independent deployment, scaling, and fault isolation. The user stories related to API access (\"As an API User, I want to be able to make queries over millions of rows in seconds\") and data processing (\"As a Data Consuming User, I need visualizations to be created quickly, even with GBs of data\") suggest that independent scaling of these components could be beneficial.\n",
      "\n",
      "**Space-Based Architecture:**\n",
      "\n",
      "Reasoning: The user stories \"As an API User, I want to have consistently high download/upload speed\", \"As an API User, I want the API service to be available 24/7\", \"As a Data Consuming User, I need visualizations to be created quickly, even with GBs of data.\" and \"As an API User, I want to be able to make queries over millions of rows in seconds\" suggest that the project needs to handle large datasets and high traffic. Space-based architecture is designed to handle these types of requirements. The ability to distribute data and processing across multiple nodes to handle high traffic and large datasets aligns well with the project's requirements.\n",
      "\n",
      "**Pipeline Architecture:**\n",
      "\n",
      "Reasoning: A pipeline architecture could be suitable for the data ingestion and processing aspects of the Open Spending project. Data could flow through a pipeline of stages, such as data validation, cleansing, transformation, and indexing. The user stories related to data import (\"As a Data Publishing User, I want to be able to import data in Excel, JSON, Google Spreadsheet\") and data validation (\"As a Data Publishing User, I want to know if my CSV file is valid\") support this idea.\n",
      "\n",
      "**Client-Server Architecture:**\n",
      "\n",
      "Reasoning: The Open Spending project is inherently a client-server application. Users interact with the system through a web browser (client), and the server handles requests, data processing, and storage. The API also follows a client-server model. This pattern is fundamental to the project's architecture.\n",
      "\n",
      "Final Scores in JSON format:\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"Layered Architecture\": \"Well suited\",\n",
      "  \"Event-Driven Architecture\": \"Partially suitable\",\n",
      "  \"Microkernel Architecture\": \"Completely unsuitable\",\n",
      "  \"Microservices Architecture\": \"Well suited\",\n",
      "  \"Space-Based Architecture\": \"Well suited\",\n",
      "  \"Pipeline Architecture\": \"Partially suitable\",\n",
      "  \"Client-Server Architecture\": \"Perfectly aligned\"\n",
      "}\n",
      "```\n",
      "\n",
      "\n",
      "=== Evaluator Feedback (Iteration 2) ===\n",
      "I have reviewed the updated reasoning and scores. Everything looks good.\n",
      "\n",
      "NO_REFINEMENT\n",
      "\n",
      "Evaluator says no changes are needed. Stopping refinement.\n",
      "=== Final Refined Scores & Reasoning ===\n",
      " Okay, I will refine the previous answer based on the evaluator's feedback, specifically increasing the score for Space-Based Architecture to \"Well suited\".\n",
      "\n",
      "**Layered Architecture:**\n",
      "\n",
      "Reasoning: The Open Spending project can benefit from a layered architecture. The application has distinct layers such as the presentation layer (user interface for data visualization and interaction), the business logic layer (data processing, aggregation, currency conversion, and API handling), and the data persistence layer (storage of datasets and metadata). User stories like \"As a Data Consuming User, I want to be able to filter, sort and aggregate data by multiple dimensions and measures\" and \"As an API User, I want to be able to get a set of monetary measures transferred to different currencies\" highlight the need for a business logic layer. The separation of concerns offered by a layered architecture will improve maintainability, testability, and scalability.\n",
      "\n",
      "**Event-Driven Architecture:**\n",
      "\n",
      "Reasoning: While not explicitly stated, an event-driven architecture could be useful for certain aspects of the Open Spending project. For example, data updates or uploads could trigger events that initiate data processing, indexing, or notification processes. The user story \"As a Data Publishing User, I want to have my dataset update automatically as the source file/files changes\" could be implemented using an event-driven approach. However, the core functionality doesn't heavily rely on asynchronous events, so it's not a primary architectural driver.\n",
      "\n",
      "**Microkernel Architecture:**\n",
      "\n",
      "Reasoning: The Open Spending project doesn't seem to fit the microkernel architecture pattern very well. The core functionalities, such as data import, processing, visualization, and API access, are relatively well-defined and integrated. There isn't a clear need to separate a minimal core system from optional plug-in modules. While some features like specific data import formats or visualization types could potentially be implemented as plugins, it's not a central requirement.\n",
      "\n",
      "**Microservices Architecture:**\n",
      "\n",
      "Reasoning: The Open Spending project could potentially benefit from a microservices architecture. Different functionalities, such as data ingestion, data processing/normalization, API, visualization, user management, and search, could be implemented as independent microservices. This would allow for independent deployment, scaling, and fault isolation. The user stories related to API access (\"As an API User, I want to be able to make queries over millions of rows in seconds\") and data processing (\"As a Data Consuming User, I need visualizations to be created quickly, even with GBs of data\") suggest that independent scaling of these components could be beneficial.\n",
      "\n",
      "**Space-Based Architecture:**\n",
      "\n",
      "Reasoning: The user stories \"As an API User, I want to have consistently high download/upload speed\", \"As an API User, I want the API service to be available 24/7\", \"As a Data Consuming User, I need visualizations to be created quickly, even with GBs of data.\" and \"As an API User, I want to be able to make queries over millions of rows in seconds\" suggest that the project needs to handle large datasets and high traffic. Space-based architecture is designed to handle these types of requirements. The ability to distribute data and processing across multiple nodes to handle high traffic and large datasets aligns well with the project's requirements.\n",
      "\n",
      "**Pipeline Architecture:**\n",
      "\n",
      "Reasoning: A pipeline architecture could be suitable for the data ingestion and processing aspects of the Open Spending project. Data could flow through a pipeline of stages, such as data validation, cleansing, transformation, and indexing. The user stories related to data import (\"As a Data Publishing User, I want to be able to import data in Excel, JSON, Google Spreadsheet\") and data validation (\"As a Data Publishing User, I want to know if my CSV file is valid\") support this idea.\n",
      "\n",
      "**Client-Server Architecture:**\n",
      "\n",
      "Reasoning: The Open Spending project is inherently a client-server application. Users interact with the system through a web browser (client), and the server handles requests, data processing, and storage. The API also follows a client-server model. This pattern is fundamental to the project's architecture.\n",
      "\n",
      "Final Scores in JSON format:\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"Layered Architecture\": \"Well suited\",\n",
      "  \"Event-Driven Architecture\": \"Partially suitable\",\n",
      "  \"Microkernel Architecture\": \"Completely unsuitable\",\n",
      "  \"Microservices Architecture\": \"Well suited\",\n",
      "  \"Space-Based Architecture\": \"Well suited\",\n",
      "  \"Pipeline Architecture\": \"Partially suitable\",\n",
      "  \"Client-Server Architecture\": \"Perfectly aligned\"\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Summarize the evaluation result"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "source": [
    "refiner_context.append({'role': 'system', 'content': \"\"\"\n",
    "Ok now the evaluation process is finished. take the first assessment and the last assessment from the user. \n",
    "And just return the final scores before and after the evaluation-refinement process\n",
    "following this json format below:\n",
    "\n",
    "{\n",
    "  \"Layered Architecture\": {\"before\": \"score option\", \"after\": \"score option\"},\n",
    "  \"Event-Driven Architecture\": {\"before\": \"score option\", \"after\": \"score option\"},\n",
    "  \"Microkernel Architecture\": {\"before\": \"score option\", \"after\": \"score option\"},\n",
    "  \"Microservices Architecture\": {\"before\": \"score option\", \"after\": \"score option\"},\n",
    "  \"Space-Based Architecture\": {\"before\": \"score option\", \"after\": \"score option\"},\n",
    "  \"Pipeline Architecture\": {\"before\": \"score option\", \"after\": \"score option\"},\n",
    "  \"Client-Server Architecture\": {\"before\": \"score option\", \"after\": \"score option\"}\n",
    "}\n",
    "\"\"\"})\n",
    "\n",
    "refiner_context.append({'role': 'user', 'content': f\"\"\"\n",
    "read the first assessment and the last assessment, and return the final scores before and after the evaluation-refinement process\n",
    "following this json format below:\n",
    "\n",
    "{{\n",
    "  \"Layered Architecture\": {{\"before\": \"score option\", \"after\": \"score option\"}},\n",
    "  \"Event-Driven Architecture\": {{\"before\": \"score option\", \"after\": \"score option\"}},\n",
    "  \"Microkernel Architecture\": {{\"before\": \"score option\", \"after\": \"score option\"}},\n",
    "  \"Microservices Architecture\": {{\"before\": \"score option\", \"after\": \"score option\"}},\n",
    "  \"Space-Based Architecture\": {{\"before\": \"score option\", \"after\": \"score option\"}},\n",
    "  \"Pipeline Architecture\": {{\"before\": \"score option\", \"after\": \"score option\"}},\n",
    "  \"Client-Server Architecture\": {{\"before\": \"score option\", \"after\": \"score option\"}}\n",
    "}}\n",
    "\n",
    "\n",
    "first assessment:\n",
    "{refiner_context[2].content.strip()}\n",
    "\n",
    "last assessment:\n",
    "{current_answer.strip()}\n",
    "\"\"\"})\n",
    "response = client.chat.completions.create(model=model_name, \n",
    "    messages=refiner_context, temperature=temperature, top_p=top_p, max_completion_tokens=num_ctx) # need deterministic answer\n",
    "message = response.choices[0].message\n",
    "refiner_context.append(message)\n",
    "print(message.content)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "```json\n",
      "{\n",
      "  \"Layered Architecture\": {\"before\": \"Well suited\", \"after\": \"Well suited\"},\n",
      "  \"Event-Driven Architecture\": {\"before\": \"Partially suitable\", \"after\": \"Partially suitable\"},\n",
      "  \"Microkernel Architecture\": {\"before\": \"Completely unsuitable\", \"after\": \"Completely unsuitable\"},\n",
      "  \"Microservices Architecture\": {\"before\": \"Well suited\", \"after\": \"Well suited\"},\n",
      "  \"Space-Based Architecture\": {\"before\": \"Moderately applicable\", \"after\": \"Well suited\"},\n",
      "  \"Pipeline Architecture\": {\"before\": \"Partially suitable\", \"after\": \"Partially suitable\"},\n",
      "  \"Client-Server Architecture\": {\"before\": \"Perfectly aligned\", \"after\": \"Perfectly aligned\"}\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "source": [
    "import json\n",
    "\n",
    "def parse_json_in_string(s):\n",
    "    \"\"\"\n",
    "    Parses and returns the first JSON object found in the input string.\n",
    "\n",
    "    Args:\n",
    "        s (str): The input string that contains at least one JSON object.\n",
    "\n",
    "    Returns:\n",
    "        object: The Python representation of the parsed JSON object.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If no JSON object is found or if decoding fails.\n",
    "    \"\"\"\n",
    "    # Find the first occurrence of '{'\n",
    "    start = s.find('{')\n",
    "    if start == -1:\n",
    "         return None\n",
    "\n",
    "    decoder = json.JSONDecoder()\n",
    "    try:\n",
    "        obj, _ = decoder.raw_decode(s, idx=start)\n",
    "        return obj\n",
    "    except json.JSONDecodeError as e:\n",
    "        return None"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Calculate weighted Kappa score"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "source": [
    "from sklearn.metrics import cohen_kappa_score\n",
    "architecture_patterns = [\"Layered Architecture\", \"Event-Driven Architecture\",\n",
    " \"Microkernel Architecture\", \"Microservices Architecture\", \"Space-Based Architecture\", \"Pipeline Architecture\", \"Client-Server Architecture\"]\n",
    "\n",
    "refiner_context = [message if isinstance(message, dict) else message.dict() for message in refiner_context]\n",
    "\n",
    "scores = {\"Completely unsuitable\": 1, \"Partially suitable\": 2, \"Moderately applicable\": 3, \"Well suited\": 4, \"Perfectly aligned\": 5}\n",
    "with open('expert_answers.json') as f:\n",
    "    expert_answers = json.load(f)\n",
    "llm_answers = parse_json_in_string(refiner_context[-1][\"content\"])\n",
    "answers_before = [scores[llm_answers[pattern][\"before\"]] for pattern in architecture_patterns]\n",
    "answers_after = [scores[llm_answers[pattern][\"after\"]] for pattern in architecture_patterns]\n",
    "score_before_refinement = cohen_kappa_score(expert_answers[selected_project], answers_before, labels=[1, 2, 3, 4, 5], weights='quadratic')\n",
    "score_after_refinement = cohen_kappa_score(expert_answers[selected_project], answers_after, labels=[1, 2, 3, 4, 5], weights='quadratic')\n",
    "\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/var/folders/r9/r79c48353r5cvd6d3tz920_h0000gn/T/ipykernel_65431/564283655.py:5: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
      "  refiner_context = [message if isinstance(message, dict) else message.dict() for message in refiner_context]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Print log to output file"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "source": [
    "from datetime import datetime\n",
    "import json\n",
    "from pathlib import Path\n",
    "Path(\"./logs/self-refinement-one-shot\").mkdir(parents=True, exist_ok=True)\n",
    "Path(\"./logs/self-refinement-zero-shot\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "final_data = {\n",
    "        \"modelName\": model_name,\n",
    "        \"temperature\": temperature,\n",
    "        \"context_limit\": num_ctx,\n",
    "        \"projectTitle\": project_title,\n",
    "        \"file_name\": file_name,\n",
    "        \"selfRefinement\": True,\n",
    "        \"oneShot\": one_shot,\n",
    "        \"wckScoreBeforeRefinement\": score_before_refinement,\n",
    "        \"wckScoreAfterRefinement\": score_after_refinement,\n",
    "        \"patternDescriptionsAdded\": add_pattern_descriptions,\n",
    "        \"timestamp\": datetime.now().strftime(\"%Y%m%d_%H%M%S\"),\n",
    "        \"json_output_parsed\": parse_json_in_string(refiner_context[-1][\"content\"]),\n",
    "        \"numberOfIterations\": refinement_iterations,\n",
    "        \"maxIterationsAllowed\": MAX_REFINEMENT_ITERATIONS,\n",
    "        \"messages\": [message[\"content\"] for message in refiner_context],\n",
    "        \"scoreGenerationDuration\": score_generation_duration,\n",
    "        \"refinementDuration\": refinement_duration\n",
    "    }\n",
    "    \n",
    "# 3) Generate a filename based on model name and current timestamp\n",
    "if one_shot:\n",
    "    filename = f\"./logs/self-refinement-one-shot/log_{model_version}_{final_data['timestamp']}.json\"\n",
    "else:\n",
    "    filename = f\"./logs/self-refinement-zero-shot/log_{model_version}_{final_data['timestamp']}.json\"\n",
    "# 4) Write the conversation to a JSON file\n",
    "with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(final_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"Assesment complete. The whole conversation is saved to {filename}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Assesment complete. The whole conversation is saved to ./logs/self-refinement-one-shot/log_gemini-2.0-flash_20250206_054843.json\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.10 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "interpreter": {
   "hash": "95ec9ec1504d83f612128e0fb229072f90bbb4cb09d9d5d93b5dd26e0ca2cfd1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}